#Repeat with correct dataset
RMANOVA.Tissue.age.Rx.correct <- aov(Uptake ~ Tissue*PN1_Rx*age + Error(sample), data=DNAandProtein_long_true)
summary(RMANOVA.Tissue.age.Rx.correct)
DNAandProtein_long_true
Dataset1.na2 <- readr::read_csv("/Users/pol/Desktop/Dataset1.csv")
Dataset1.na2 <- Dataset1.na2[Dataset1.na2$synthesis == "dna" ,]
#Convert correct dataset to long form
DNAandProtein_long_true <- melt(Dataset1.na2,
id.vars=c("ID", "PN1_Rx", "age"),
measure.vars=c("heart_incorp_uptake", "liver_incorp_uptake"),
variable.name="Tissue",
value.name="Uptake"
)
#Repeat with correct dataset
RMANOVA.Tissue.age.Rx.correct <- aov(Uptake ~ Tissue*PN1_Rx*age + Error(ID), data=DNAandProtein_long_true)
summary(RMANOVA.Tissue.age.Rx.correct)
#Repeat with correct dataset
RMANOVA.Tissue.age.Rx.correct <- aov(Uptake ~ Tissue*PN1_Rx*age + Error(ID/Tissue), data=DNAandProtein_long_true)
summary(RMANOVA.Tissue.age.Rx.correct)
#Repeat with correct dataset
RMANOVA.Tissue.age.Rx.correct <- aov(Uptake ~ Tissue*PN1_Rx*age + Error(ID/Tissue*PN1_Rx*age), data=DNAandProtein_long_true)
summary(RMANOVA.Tissue.age.Rx.correct)
x <- mtcars[,1:6]
y <- mtcars$qsec
#' @export
#'
#' @return A LassoLoop object with the results.
#' @references Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL http://www.jstatsoft.org/v33/i01/.
#' @author Pol Castellano-Escuder
#'
#' @importFrom tictoc tic toc
#' @importFrom doParallel registerDoParallel
#' @importFrom foreach foreach %dopar%
#' @importFrom glmnet cv.glmnet
blasso <- function(x,
y,
loops = 2,
bootstrap = TRUE,
alpha = 1,
nfolds = 10,
offset = NULL,
family = "gaussian",
seed = 987654321,
ncores = 2){
tictoc::tic()
doParallel::registerDoParallel(cores = ncores)
set.seed(seed)
varx <- colnames(x)
rowx <- nrow(x)
nvar <- ncol(x)
n <- length(y)
res <- vector("list", loops)
if(rowx != n){
stop("The number of rows in x is not equal to the length of y!")
}
res <- foreach::foreach(i = 1:loops) %dopar% {
## BOOTSTRAP
if(isTRUE(bootstrap)){
idx <- sample(1:n, replace = T)
new_matrix <- cbind(y, x)
new_matrix <- new_matrix[idx ,]
} else{
new_matrix <- cbind(y, x)
}
## TEST
idx_test <- sample(1:n, 0.2*n, replace = FALSE)
test <- new_matrix[idx_test ,]
test_x <- test[,-1]
test_y <- test[,1]
## TRAIN
train <- new_matrix[-idx_test ,]
train_x <- train[,-1]
train_y <- train[,1]
## LASSO
cv_fit <- glmnet::cv.glmnet(data.matrix(train_x), as.matrix(train_y), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
tmp_coeffs <- coef(cv_fit, s = "lambda.min")
final_coef <- data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x))
mse <- mean((test_y - lasso_pred)^2)
res[[i]] <- list(coeffs = final_coef, mse = mse, model = cv_fit)
}
res <- new("LassoLoop",
model = purrr::map(res, 3),
bootstraped = bootstrap,
coefficients = purrr::map(res, 1),
family = family,
offset = offset,
valiadationMetric = "Mean Square Error",
valiadationValues = purrr::map(res, 2),
length = length(res))
tictoc::toc()
if(validObject(res))
return(res)
return(res)
}
blasso(x, y, loops = 10, bootstrap = TRUE, ncores = 1, offset = log(rep(0.5,length(y))))
library(doParallel)
blasso(x, y, loops = 10, bootstrap = TRUE, ncores = 1, offset = log(rep(0.5,length(y))))
loops = 2
bootstrap = TRUE
alpha = 1
nfolds = 10
family = "gaussian"
blasso(x, y, loops = 10, family = "poisson", bootstrap = TRUE, ncores = 1, offset = log(rep(0.5,length(y))))
family <- "poisson"
seed = 987654321
ncores = 2
set.seed(seed)
varx <- colnames(x)
rowx <- nrow(x)
nvar <- ncol(x)
n <- length(y)
res <- vector("list", loops)
if(rowx != n){
stop("The number of rows in x is not equal to the length of y!")
}
res <- foreach::foreach(i = 1:loops) %dopar% {
## BOOTSTRAP
if(isTRUE(bootstrap)){
idx <- sample(1:n, replace = T)
new_matrix <- cbind(y, x)
new_matrix <- new_matrix[idx ,]
} else{
new_matrix <- cbind(y, x)
}
## TEST
idx_test <- sample(1:n, 0.2*n, replace = FALSE)
test <- new_matrix[idx_test ,]
test_x <- test[,-1]
test_y <- test[,1]
## TRAIN
train <- new_matrix[-idx_test ,]
train_x <- train[,-1]
train_y <- train[,1]
## LASSO
cv_fit <- glmnet::cv.glmnet(data.matrix(train_x), as.matrix(train_y), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
tmp_coeffs <- coef(cv_fit, s = "lambda.min")
final_coef <- data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x))
mse <- mean((test_y - lasso_pred)^2)
res[[i]] <- list(coeffs = final_coef, mse = mse, model = cv_fit)
}
idx <- sample(1:n, replace = T)
new_matrix <- cbind(y, x)
new_matrix <- new_matrix[idx ,]
idx_test <- sample(1:n, 0.2*n, replace = FALSE)
test <- new_matrix[idx_test ,]
test_x <- test[,-1]
test_y <- test[,1]
train <- new_matrix[-idx_test ,]
train_x <- train[,-1]
train_y <- train[,1]
cv_fit <- glmnet::cv.glmnet(data.matrix(train_x), as.matrix(train_y), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
offset = log(rep(0.5,length(y)))
cv_fit <- glmnet::cv.glmnet(data.matrix(train_x), as.matrix(train_y), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
tmp_coeffs <- coef(cv_fit, s = "lambda.min")
final_coef <- data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x))
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x), newoffset = offset)
mse <- mean((test_y - lasso_pred)^2)
list(coeffs = final_coef, mse = mse, model = cv_fit)
#' @export
#'
#' @return A LassoLoop object with the results.
#' @references Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL http://www.jstatsoft.org/v33/i01/.
#' @author Pol Castellano-Escuder
#'
#' @importFrom tictoc tic toc
#' @importFrom doParallel registerDoParallel
#' @importFrom foreach foreach %dopar%
#' @importFrom glmnet cv.glmnet
blasso <- function(x,
y,
loops = 2,
bootstrap = TRUE,
alpha = 1,
nfolds = 10,
offset = NULL,
family = "gaussian",
seed = 987654321,
ncores = 2){
tictoc::tic()
doParallel::registerDoParallel(cores = ncores)
set.seed(seed)
varx <- colnames(x)
rowx <- nrow(x)
nvar <- ncol(x)
n <- length(y)
res <- vector("list", loops)
if(rowx != n){
stop("The number of rows in x is not equal to the length of y!")
}
res <- foreach::foreach(i = 1:loops) %dopar% {
## BOOTSTRAP
if(bootstrap){
idx <- sample(1:n, replace = T)
new_matrix <- cbind(y, x)
new_matrix <- new_matrix[idx ,]
} else{
new_matrix <- cbind(y, x)
}
## TEST
idx_test <- sample(1:n, 0.2*n, replace = FALSE)
test <- new_matrix[idx_test ,]
test_x <- test[,-1]
test_y <- test[,1]
## TRAIN
train <- new_matrix[-idx_test ,]
train_x <- train[,-1]
train_y <- train[,1]
## LASSO
cv_fit <- glmnet::cv.glmnet(data.matrix(train_x), as.matrix(train_y), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
tmp_coeffs <- coef(cv_fit, s = "lambda.min")
final_coef <- data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
if(!is.null(offset)) {
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x), newoffset = offset)
}
else {
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x))
}
mse <- mean((test_y - lasso_pred)^2)
res[[i]] <- list(coeffs = final_coef, mse = mse, model = cv_fit)
}
res <- new("LassoLoop",
model = purrr::map(res, 3),
bootstraped = bootstrap,
coefficients = purrr::map(res, 1),
family = family,
offset = offset,
valiadationMetric = "Mean Square Error",
valiadationValues = purrr::map(res, 2),
length = length(res))
tictoc::toc()
if(validObject(res))
return(res)
return(res)
}
blasso(x, y, loops = 10, family = "poisson", bootstrap = TRUE, ncores = 1, offset = log(rep(0.5,length(y))))
#' @export
#'
#' @return A LassoLoop object with the results.
#' @references Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL http://www.jstatsoft.org/v33/i01/.
#' @author Pol Castellano-Escuder
#'
#' @importFrom tictoc tic toc
#' @importFrom doParallel registerDoParallel
#' @importFrom foreach foreach %dopar%
#' @importFrom glmnet cv.glmnet
blasso <- function(x,
y,
loops = 2,
bootstrap = TRUE,
alpha = 1,
nfolds = 10,
offset = NULL,
family = "gaussian",
seed = 987654321,
ncores = 2){
tictoc::tic()
doParallel::registerDoParallel(cores = ncores)
set.seed(seed)
varx <- colnames(x)
rowx <- nrow(x)
nvar <- ncol(x)
n <- length(y)
res <- vector("list", loops)
if(rowx != n){
stop("The number of rows in x is not equal to the length of y!")
}
res <- foreach::foreach(i = 1:loops) %dopar% {
## BOOTSTRAP
if(bootstrap){
idx <- sample(1:n, replace = T)
new_matrix <- cbind(y, x)
new_matrix <- new_matrix[idx ,]
} else{
new_matrix <- cbind(y, x)
}
## TEST
idx_test <- sample(1:n, 0.2*n, replace = FALSE)
test <- new_matrix[idx_test ,]
test_x <- test[,-1]
test_y <- test[,1]
## TRAIN
train <- new_matrix[-idx_test ,]
train_x <- train[,-1]
train_y <- train[,1]
## LASSO
cv_fit <- glmnet::cv.glmnet(data.matrix(train_x), as.matrix(train_y), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
tmp_coeffs <- coef(cv_fit, s = "lambda.min")
final_coef <- data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
if(!is.null(offset)) {
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x), newoffset = offset)
}
else {
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x))
}
mse <- mean((test_y - lasso_pred)^2)
res[[i]] <- list(coeffs = final_coef, mse = mse, model = cv_fit)
}
res <- new("LassoLoop",
model = purrr::map(res, 3),
bootstraped = bootstrap,
coefficients = purrr::map(res, 1),
family = family,
valiadationMetric = "Mean Square Error",
valiadationValues = purrr::map(res, 2),
length = length(res))
tictoc::toc()
if(validObject(res))
return(res)
return(res)
}
blasso(x, y, loops = 10, family = "poisson", bootstrap = TRUE, ncores = 1, offset = log(rep(0.5,length(y))))
cv_fit <- glmnet::cv.glmnet(data.matrix(train_x), as.matrix(train_y), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
#' @export
#'
#' @return A LassoLoop object with the results.
#' @references Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL http://www.jstatsoft.org/v33/i01/.
#' @author Pol Castellano-Escuder
#'
#' @importFrom tictoc tic toc
#' @importFrom doParallel registerDoParallel
#' @importFrom foreach foreach %dopar%
#' @importFrom glmnet cv.glmnet
blasso <- function(x,
y,
loops = 2,
bootstrap = TRUE,
alpha = 1,
nfolds = 10,
offset = NULL,
family = "gaussian",
seed = 987654321,
ncores = 2){
tictoc::tic()
doParallel::registerDoParallel(cores = ncores)
set.seed(seed)
varx <- colnames(x)
rowx <- nrow(x)
nvar <- ncol(x)
n <- length(y)
res <- vector("list", loops)
if(rowx != n){
stop("The number of rows in x is not equal to the length of y!")
}
res <- foreach::foreach(i = 1:loops) %dopar% {
## BOOTSTRAP
if(bootstrap){
idx <- sample(1:n, replace = T)
new_matrix <- cbind(y, x)
new_matrix <- new_matrix[idx ,]
} else{
new_matrix <- cbind(y, x)
}
## TEST
idx_test <- sample(1:n, 0.2*n, replace = FALSE)
test <- new_matrix[idx_test ,]
test_x <- test[,-1]
test_y <- test[,1]
## TRAIN
train <- new_matrix[-idx_test ,]
train_x <- train[,-1]
train_y <- train[,1]
## LASSO
suppressWarnings(
cv_fit <- glmnet::cv.glmnet(data.matrix(train_x), as.matrix(train_y), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
)
tmp_coeffs <- coef(cv_fit, s = "lambda.min")
final_coef <- data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
if(!is.null(offset)) {
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x), newoffset = offset)
}
else {
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x))
}
mse <- mean((test_y - lasso_pred)^2)
res[[i]] <- list(coeffs = final_coef, mse = mse, model = cv_fit)
}
res <- new("LassoLoop",
model = purrr::map(res, 3),
bootstraped = bootstrap,
coefficients = purrr::map(res, 1),
family = family,
valiadationMetric = "Mean Square Error",
valiadationValues = purrr::map(res, 2),
length = length(res))
tictoc::toc()
if(validObject(res))
return(res)
return(res)
}
blasso(x, y, loops = 10, family = "poisson", bootstrap = TRUE, ncores = 1, offset = log(rep(0.5,length(y))))
blasso(x, y, loops = 10, family = "poisson", bootstrap = TRUE, ncores = 1)
blasso(x, y, loops = 10, family = "poisson", bootstrap = TRUE, ncores = 1, offset = NULL)
tt <- blasso(x, y, loops = 10, family = "poisson", bootstrap = TRUE, ncores = 1, offset = NULL)
lassoloops::summary_models(tt)
blasso(x, y, loops = 10, family = "poisson", bootstrap = TRUE, ncores = 1, offset = log(rep(0.5, length(y))))
devtools::load_all()
devtools::document()
devtools::load_all()
devtools::document()
#' @export
#'
#' @return A LassoLoop object with the results.
#' @references Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL http://www.jstatsoft.org/v33/i01/.
#' @author Pol Castellano-Escuder
#'
#' @importFrom tictoc tic toc
#' @importFrom doParallel registerDoParallel
#' @importFrom foreach foreach %dopar%
#' @importFrom glmnet cv.glmnet
blasso <- function(x,
y,
loops = 2,
bootstrap = TRUE,
alpha = 1,
nfolds = 10,
offset = NULL,
family = "gaussian",
ntest = NULL,
seed = 987654321,
ncores = 2){
tictoc::tic()
doParallel::registerDoParallel(cores = ncores)
set.seed(seed)
varx <- colnames(x)
rowx <- nrow(x)
nvar <- ncol(x)
n <- length(y)
res <- vector("list", loops)
if(rowx != n){
stop("The number of rows in x is not equal to the length of y!")
}
res <- foreach::foreach(i = 1:loops) %dopar% {
## BOOTSTRAP
if(bootstrap){
idx <- sample(1:n, replace = T)
new_matrix <- cbind(y, x)
new_matrix <- new_matrix[idx ,]
} else {
new_matrix <- cbind(y, x)
}
if(!is.null(ntest)){
## TEST
idx_test <- sample(1:n, 0.2*n, replace = FALSE)
test <- new_matrix[idx_test ,]
test_x <- test[,-1]
test_y <- test[,1]
## TRAIN
train <- new_matrix[-idx_test ,]
train_x <- train[,-1]
train_y <- train[,1]
## LASSO
suppressWarnings(
cv_fit <- glmnet::cv.glmnet(data.matrix(train_x), as.matrix(train_y), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
)
} else {
suppressWarnings(
cv_fit <- glmnet::cv.glmnet(data.matrix(new_matrix[,-1]), as.matrix(new_matrix[,1]), alpha = alpha,
family = family, nfolds = nfolds,
offset = offset)
)
}
tmp_coeffs <- coef(cv_fit, s = "lambda.min")
final_coef <- data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
if(!is.null(offset)) {
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x), newoffset = offset)
}
else {
lasso_pred <- predict(cv_fit, s = cv_fit$lambda.min, newx = data.matrix(test_x))
}
if(!is.null(ntest)){
mse <- mean((test_y - lasso_pred)^2)
} else {
mse <- NULL
}
res[[i]] <- list(coeffs = final_coef, mse = mse, model = cv_fit)
}
res <- new("LassoLoop",
model = purrr::map(res, 3),
bootstraped = bootstrap,
coefficients = purrr::map(res, 1),
family = family,
valiadationMetric = "Mean Square Error",
valiadationValues = purrr::map(res, 2),
length = length(res))
tictoc::toc()
if(validObject(res))
return(res)
return(res)
}
devtools::load_all()
devtools::document()
